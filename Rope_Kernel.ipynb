{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fbv837BpkRvR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original"
      ],
      "metadata": {
        "id": "8Liy-5_xBwJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Rotary Position Embedding from https://arxiv.org/abs/2104.09864.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        rotary_percent: float = 1.0,\n",
        "        seq_len_interpolation_factor: Optional[int] = None,\n",
        "        pretrained_max_position_embeddings: Optional[int] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        dim: int\n",
        "            rotary embedding dimension\n",
        "        rotary_percent: float\n",
        "            Percent of rotary dimension to use for rotary position embeddings.\n",
        "        seq_len_interpolation_factor: int\n",
        "            if not None, discrete positions will be interpolated by this factor via the trick in\n",
        "            https://arxiv.org/abs/2306.15595\n",
        "        pretrained_max_position_embeddings: int\n",
        "            pre-trained max_position_embeddings before position interpolation\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if rotary_percent < 1.0:\n",
        "            dim = int(dim * rotary_percent)\n",
        "        self.seq_len_interpolation_factor = seq_len_interpolation_factor\n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (\n",
        "                torch.arange(0, dim, 2, dtype=torch.float32, device=torch.cuda.current_device())\n",
        "                / dim\n",
        "            )\n",
        "        )\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.pretrained_max_position_embeddings = pretrained_max_position_embeddings\n",
        "\n",
        "    def forward(self, max_seq_len: int, offset: int = 0):\n",
        "        \"\"\"\n",
        "        Create rotary position embedding frequencies\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_seq_len: int\n",
        "            sequence length of a sample\n",
        "        offset: int, default = 0\n",
        "            fixed offset for freqencies\n",
        "        \"\"\"\n",
        "        seq = (\n",
        "            torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "            + offset\n",
        "        )\n",
        "\n",
        "        if (self.pretrained_max_position_embeddings is not None\n",
        "            and self.seq_len_interpolation_factor is not None):\n",
        "            if (max_seq_len >\n",
        "                self.pretrained_max_position_embeddings * self.seq_len_interpolation_factor):\n",
        "                # dynamic linear scaling (length > position we have learned)\n",
        "                seq *= 1 / (max_seq_len / self.pretrained_max_position_embeddings)\n",
        "            else:\n",
        "                # fixed linear scaling\n",
        "                seq *= 1 / self.seq_len_interpolation_factor\n",
        "\n",
        "        freqs = torch.einsum('i , j -> i j', seq, self.inv_freq)\n",
        "        # first part even vector components, second part odd vector components,\n",
        "        #  2 * dim in dimension size\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # emb [seq_length, .., dim]\n",
        "        return emb.reshape(emb.size(0), 1, 1, emb.size(1))"
      ],
      "metadata": {
        "id": "l8hcGnmlo1-o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    change sign so the last dimension becomes [-odd, +even]\n",
        "    \"\"\"\n",
        "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
        "    x1, x2 = x.unbind(dim=-2)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "heS3lEuskSy3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triton"
      ],
      "metadata": {
        "id": "fEC73ANWN6FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "oDeBIaqXbykT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_freqs_cis_pytorch(dim: int, end: int, theta: float = 10000.0):\n",
        "    assert dim % 2 == 0\n",
        "\n",
        "    # Generate a sequence of numbers from 0 to dim in steps of 2\n",
        "    sequence = torch.arange(0, dim, 2, dtype=torch.float32, device=\"cuda\")\n",
        "\n",
        "    # Keep only the first half of the sequence (in case dim is odd?)\n",
        "    # sequence = sequence[: (dim // 2)]\n",
        "\n",
        "    # Calculate frequency values based on the sequence and theta\n",
        "    freqs = 1.0 / (theta ** (sequence / dim))\n",
        "\n",
        "    # Create a tensor of numbers from 0 to end, it represents the position ids\n",
        "    t = torch.arange(end, device=freqs.device)\n",
        "\n",
        "    # Generate a table of frequency values\n",
        "    freqs = t[:, None] * freqs[None, :]  # torch.outer(t, freqs).float()\n",
        "\n",
        "    # Calculate cosine and sine values for the frequencies\n",
        "    # These can be considered as the real and imaginary parts of complex numbers\n",
        "    freqs_cos = torch.cos(freqs)\n",
        "    freqs_sin = torch.sin(freqs)\n",
        "\n",
        "    # Return the cosine and sine values as two separate tensors\n",
        "    return freqs_cos, freqs_sin\n",
        "\n",
        "def apply_rotary_emb_pytorch(x: torch.Tensor, freq_cos: torch.Tensor, freq_sin: torch.Tensor) -> torch.Tensor:\n",
        "    # Split x and x into real and imaginary parts\n",
        "    x_real = x[..., 0::2]\n",
        "    x_imag = x[..., 1::2]\n",
        "\n",
        "    # Reshape freq_cos and freq_sin for broadcasting\n",
        "    freq_cos = reshape_for_broadcast(freq_cos, x_real).to(torch.float32)\n",
        "    freq_sin = reshape_for_broadcast(freq_sin, x_imag).to(torch.float32)\n",
        "\n",
        "    # Perform the equivalent of complex multiplication\n",
        "    x_out_real = x_real * freq_cos - x_imag * freq_sin\n",
        "    x_out_imag = x_real * freq_sin + x_imag * freq_cos\n",
        "\n",
        "    # Combine real and imaginary parts back into the original tensor\n",
        "    x_out = torch.stack((x_out_real, x_out_imag), dim=-1).flatten(-2)\n",
        "\n",
        "    return x_out.type_as(x)\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f\"{freqs_cis.shape} != {(x.shape[1], x.shape[-1])}\"\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n"
      ],
      "metadata": {
        "id": "SO2poWlXl8XI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch, seq_len, heads, dim = [1, 16, 32, 128]\n",
        "\n",
        "embeddings_load = torch.randn([batch, seq_len, heads * dim], dtype=torch.float16, device=\"cuda\")\n",
        "rms_weights = torch.randn([heads * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n",
        "q_weights_load = torch.randn([heads * dim, heads * dim], dtype=torch.float16, device=\"cuda\") * 0.2"
      ],
      "metadata": {
        "id": "E2vwOsFYloxS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def get_freq_multi_tokens(offs_cn, starting_idx, theta: tl.constexpr, NB_TOKENS: tl.constexpr):\n",
        "    DIM: tl.constexpr = 128  # in model, dim = self.params.dim // self.params.n_heads\n",
        "    freqs = offs_cn % DIM\n",
        "    freqs = freqs.to(tl.float32) / DIM\n",
        "    freqs = tl.math.pow(theta, freqs)\n",
        "    freqs = (tl.arange(0, NB_TOKENS) + starting_idx)[:, None] / freqs[None, :]\n",
        "    return tl.cos(freqs), tl.sin(freqs)"
      ],
      "metadata": {
        "id": "zPUqSkPSrwNf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def rbe_triton(x_ptr, out_ptr,\n",
        "               M, K,\n",
        "               stride_x_batch, stride_x_m, stride_x_n,\n",
        "               stride_out_batch, stride_out_m, stride_out_n,\n",
        "               start_token_position,\n",
        "               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n",
        "    pid_batch = tl.program_id(axis=0)\n",
        "    pid = tl.program_id(axis=1)\n",
        "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n",
        "    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n",
        "    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n",
        "    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n",
        "    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n",
        "    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n",
        "    tl.debug_barrier()\n",
        "    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n",
        "    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n",
        "\n",
        "    out_real = real * cos - imag * sin\n",
        "    out_imag = real * sin + imag * cos\n",
        "    tl.debug_barrier()\n",
        "    out_ptrs = out_ptr + (\n",
        "            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n",
        "    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    tl.store(out_ptrs, out_real, mask=out_real_mask)\n",
        "    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n",
        "    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)\n",
        "\n",
        "@triton.jit\n",
        "def rbe_triton_backward(x_ptr, out_ptr,\n",
        "               M, K,\n",
        "               stride_x_batch, stride_x_m, stride_x_n,\n",
        "               stride_out_batch, stride_out_m, stride_out_n,\n",
        "               start_token_position,\n",
        "               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n",
        "    pid_batch = tl.program_id(axis=0)\n",
        "    pid = tl.program_id(axis=1)\n",
        "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n",
        "    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n",
        "    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n",
        "    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n",
        "    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n",
        "    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n",
        "    tl.debug_barrier()\n",
        "    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n",
        "    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n",
        "\n",
        "    out_real = real * cos + imag * sin\n",
        "    out_imag = -real * sin + imag * cos\n",
        "    tl.debug_barrier()\n",
        "    out_ptrs = out_ptr + (\n",
        "            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n",
        "    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    tl.store(out_ptrs, out_real, mask=out_real_mask)\n",
        "    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n",
        "    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)"
      ],
      "metadata": {
        "id": "EVvaaxkCq2vw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rbe_triton_wrapper(x: torch.Tensor, pos: int) -> torch.Tensor:\n",
        "    batch, M, K = x.shape\n",
        "    out = torch.empty_like(x)\n",
        "    grid = lambda META: (\n",
        "        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"K\"], META[\"BLOCK_SIZE_K\"]),)\n",
        "\n",
        "    rbe_triton[grid](x, out,\n",
        "                     M, K,\n",
        "                     *x.stride(),\n",
        "                     *out.stride(),\n",
        "                     start_token_position=pos, THETA=10000., BLOCK_SIZE_M=2, BLOCK_SIZE_K=1024)\n",
        "    return out\n",
        "\n",
        "def rbe_triton_wrapper_backward(x: torch.Tensor, pos: int) -> torch.Tensor:\n",
        "    batch, M, K = x.shape\n",
        "    out = torch.empty_like(x)\n",
        "    grid = lambda META: (\n",
        "        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"K\"], META[\"BLOCK_SIZE_K\"]),)\n",
        "\n",
        "    rbe_triton_backward[grid](x, out,\n",
        "                     M, K,\n",
        "                     *x.stride(),\n",
        "                     *out.stride(),\n",
        "                     start_token_position=pos, THETA=10000., BLOCK_SIZE_M=2, BLOCK_SIZE_K=1024)\n",
        "    return out, None, None,"
      ],
      "metadata": {
        "id": "ShvBp3_vpC_5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 점검 -> 통과"
      ],
      "metadata": {
        "id": "UcligTe7buEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xq = embeddings_load @ q_weights_load.t()\n",
        "xq = xq.view(batch, seq_len, heads, dim)\n",
        "\n",
        "\n",
        "xq_output_triton = embeddings_load  @ q_weights_load.t()\n",
        "out_rbe_triton = rbe_triton_wrapper(xq_output_triton, pos=0).view(batch, seq_len, heads, dim)\n",
        "\n",
        "freq_cos, freq_sin = precompute_freqs_cis_pytorch(dim=128, end=seq_len)\n",
        "out_rbe_pytorch = apply_rotary_emb_pytorch(x=xq, freq_cos=freq_cos, freq_sin=freq_sin).view(batch, seq_len, heads, dim)\n",
        "\n",
        "assert torch.allclose(out_rbe_pytorch, out_rbe_triton, atol=1e-1), f\"max diff: {torch.max(torch.abs(out_rbe_pytorch - out_rbe_triton))}\"\n",
        "print(\"rbe triton\", triton.testing.do_bench(lambda: rbe_triton_wrapper(xq_output_triton, pos=0)))\n",
        "print(\"rbe pytorch\", triton.testing.do_bench(lambda: apply_rotary_emb_pytorch(x=xq, freq_cos=freq_cos, freq_sin=freq_sin)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcJ9Kyjklo0S",
        "outputId": "4830a704-954b-4a67-d96d-eb0869144237"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rbe triton 0.016656363382935524\n",
            "rbe pytorch 0.062155671417713165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FusedRoPEFunc(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Function for FusedRoPE\n",
        "\n",
        "    This implementation assumes the input tensor to be in `sbhd`, `bshd` or `thd` format and\n",
        "    the RoPE tensor to be of shape (s, 1, 1, d). It accepts arbitrary memory layouts to avoid\n",
        "    the expensive `.contiguous()` calls, thus it may not achieve the best memory access pattern.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(\n",
        "        ctx,\n",
        "        t: torch.Tensor,\n",
        "        freqs: torch.Tensor,\n",
        "        tensor_format: str = \"bshd\",\n",
        "        cu_seqlens: Union[torch.Tensor, None] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        if tensor_format == \"sbhd\":\n",
        "            s,b,h,d = t.shape\n",
        "            output = rbe_triton_wrapper(t.transpose(0, 1).view(b,s,h*d), pos= 0).view(b,s,h,d).transpose(0, 1)\n",
        "        elif tensor_format == \"bshd\":\n",
        "            b,s,h,d = t.shape\n",
        "\n",
        "            output = rbe_triton_wrapper(\n",
        "                t.view(b,s,h*d),  pos= 0\n",
        "            ).view(b,s,h,d)\n",
        "        # elif tensor_format == \"thd\":\n",
        "        #     output = tex.fused_rope_thd_forward(t, cu_seqlens, freqs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported tensor_format: {tensor_format}.\")\n",
        "        ctx.save_for_backward(freqs, cu_seqlens)\n",
        "        ctx.tensor_format = tensor_format\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(\n",
        "        ctx, grad_output: torch.Tensor\n",
        "    ) -> Tuple[Union[torch.Tensor, None], ...]:\n",
        "        freqs, cu_seqlens = ctx.saved_tensors\n",
        "        if ctx.tensor_format == \"sbhd\":\n",
        "            s,b,h,d = grad_output.shape\n",
        "            grad_input = rbe_triton_wrapper_backward(grad_output.transpose(0, 1).view(b,s,h*d), pos= 0)[0].view(b,s,h,d).transpose(0, 1)\n",
        "        elif ctx.tensor_format == \"bshd\":\n",
        "            b,s,h,d = grad_output.shape\n",
        "            grad_input = rbe_triton_wrapper(\n",
        "                grad_output.view(b,s,h*d), pos= 0\n",
        "            )[0].view(b,s,h,d)\n",
        "        # elif ctx.tensor_format == \"thd\":\n",
        "        #     grad_input = tex.fused_rope_thd_backward(grad_output, cu_seqlens, freqs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported tensor_format: {ctx.tensor_format}.\")\n",
        "\n",
        "        return grad_input, None, None, None, None"
      ],
      "metadata": {
        "id": "-pEkKrf0mkYp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_pos_emb(\n",
        "    t: torch.Tensor,\n",
        "    freqs: torch.Tensor,\n",
        "    tensor_format: str = \"sbhd\",\n",
        "    fused: bool = False,\n",
        "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply rotary positional embedding tensor to the input tensor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    t: torch.Tensor\n",
        "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
        "        rotary positional embedding will be applied.\n",
        "    freqs: torch.Tensor\n",
        "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
        "        with `s2 >= s` and `d2 <= d`.\n",
        "    fused: bool, default = False\n",
        "        Whether to use a fused applying RoPE implementation.\n",
        "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
        "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
        "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
        "    cu_seqlens: torch.Tensor, default = None.\n",
        "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
        "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
        "    \"\"\"\n",
        "    if fused:\n",
        "        assert (\n",
        "            tensor_format != \"thd\" # or cu_seqlens is not None\n",
        "        ), \"tensor_format should not be 'thd'.\"\n",
        "        return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
        "\n",
        "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
        "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
        "        f\"when fused is False, got {tensor_format}.\"\n",
        "    )\n",
        "\n",
        "    max_seq_len = freqs.shape[0]\n",
        "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
        "\n",
        "    # Only apply the rotary embeddings up to the sequence length of the running\n",
        "    # input.\n",
        "    assert cur_seq_len <= max_seq_len, (\n",
        "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
        "    )\n",
        "    freqs = freqs[:cur_seq_len]\n",
        "    if tensor_format == \"bshd\":\n",
        "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
        "    # cos/sin first then dtype conversion for better precision\n",
        "    cos_ = torch.cos(freqs).to(t.dtype)\n",
        "    sin_ = torch.sin(freqs).to(t.dtype)\n",
        "\n",
        "    rot_dim = freqs.shape[-1]\n",
        "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
        "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
        "\n",
        "    # first part is cosine component\n",
        "    # second part is sine component, need to change signs with _rotate_half method\n",
        "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
        "    return torch.cat((t, t_pass), dim=-1)"
      ],
      "metadata": {
        "id": "Rk2dWe81mAV_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 함수"
      ],
      "metadata": {
        "id": "cJxFrK95aRlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fused_rope(\n",
        "    dtype: torch.dtype,\n",
        "    seq_length: int,\n",
        "    hidden_size: int,\n",
        "    rotary_percent: float,\n",
        "    margin: int,\n",
        "    transpose: Union[Tuple, None],\n",
        "    tensor_format: str,\n",
        "    loss_func: Callable,\n",
        "\n",
        ") -> None:\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    batch_size, head_num = 2, 64\n",
        "    t = torch.rand(\n",
        "        (seq_length - margin, batch_size, head_num, hidden_size),\n",
        "        dtype=dtype,\n",
        "        device=device,\n",
        "    )\n",
        "    if tensor_format == \"bshd\":\n",
        "        t = t.transpose(0, 1).contiguous()\n",
        "    if transpose:\n",
        "        t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
        "    t.requires_grad = True\n",
        "\n",
        "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
        "    emb = rotary_pos_emb(seq_length)\n",
        "\n",
        "\n",
        "    # unfused\n",
        "    output_unfused = apply_rotary_pos_emb(\n",
        "        t,\n",
        "        emb,\n",
        "        tensor_format=tensor_format,\n",
        "        fused=False\n",
        "    )\n",
        "    loss_unfused = loss_func(output_unfused)\n",
        "    loss_unfused.backward()\n",
        "    grad_unfused = t.grad.detach().clone()\n",
        "    t.grad = None\n",
        "\n",
        "\n",
        "    # fused\n",
        "    output_fused = apply_rotary_pos_emb(\n",
        "        t,\n",
        "        emb,\n",
        "        tensor_format=tensor_format,\n",
        "        fused=True,\n",
        "    )\n",
        "    loss_fused = loss_func(output_fused)\n",
        "    loss_fused.backward()\n",
        "    grad_fused = t.grad.detach().clone()\n",
        "    t.grad = None\n",
        "\n",
        "    torch.testing.assert_close(output_fused, output_unfused, **get_tol(dtype))\n",
        "    torch.testing.assert_close(grad_fused, grad_unfused, **get_tol(dtype))\n",
        "    assert output_fused.is_contiguous()\n"
      ],
      "metadata": {
        "id": "BM_UVKKMvxZB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient is a broadcasted scalar\n",
        "def _overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
        "    return output.sum() * 2\n",
        "\n",
        "# Gradient is a full tensor\n",
        "def _non_overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
        "    t = torch.ones_like(output)\n",
        "    return torch.sum(output * t)\n",
        "\n",
        "def get_tol(dtype: torch.dtype) -> Dict:\n",
        "    if dtype == torch.bfloat16:\n",
        "        return dict(atol=1e-2, rtol=1e-2)\n",
        "    elif dtype == torch.float16:\n",
        "        return dict(atol=1e-3, rtol=1e-3)\n",
        "    return dict(atol=1e-5, rtol=1.3e-6)\n"
      ],
      "metadata": {
        "id": "Mgf1DYvFxl12"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과 비교\n",
        "테스트 함수에서 다른 결과가 나오는 문제 발생..."
      ],
      "metadata": {
        "id": "Qkc-dseaZ-M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotary_pos_emb = RotaryPositionEmbedding(dim)\n",
        "emb = rotary_pos_emb(seq_len)\n",
        "test_input = torch.randn([batch, seq_len, heads , dim], dtype=torch.float16, device=\"cuda\")\n",
        "out_rbe_triton = apply_rotary_pos_emb(test_input, emb, tensor_format= 'sbhd', fused = True).view(batch, seq_len, heads, dim)\n",
        "out_rbe_pytorch = apply_rotary_pos_emb(test_input, emb, tensor_format= 'sbhd', fused = False).view(batch, seq_len, heads, dim)\n",
        "torch.testing.assert_close(out_rbe_triton, out_rbe_pytorch, **get_tol(torch.float16)) # 이 결과는 동일한데.."
      ],
      "metadata": {
        "id": "sVSBAdReB1L6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_fused_rope(torch.float16, 1024, 128, 1.0, 0, None, \"sbhd\", _overlapping_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RusxtX6gxdaZ",
        "outputId": "d9e49d69-746a-4c49-92a0-e8580f93dac3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Tensor-likes are not close!\n\nMismatched elements: 16656072 / 16777216 (99.3%)\nGreatest absolute difference: 2.79296875 at index (490, 1, 7, 91) (up to 0.001 allowed)\nGreatest relative difference: inf at index (1, 1, 25, 7) (up to 0.001 allowed)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7509c85edd30>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_fused_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sbhd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_overlapping_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-bf7b08593e6d>\u001b[0m in \u001b[0;36mtest_fused_rope\u001b[0;34m(dtype, seq_length, hidden_size, rotary_percent, margin, transpose, tensor_format, loss_func)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_fused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_unfused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mget_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_unfused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mget_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0moutput_fused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/testing/_comparison.py\u001b[0m in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_metas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;31m# TODO: compose all metas into one AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror_metas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 16656072 / 16777216 (99.3%)\nGreatest absolute difference: 2.79296875 at index (490, 1, 7, 91) (up to 0.001 allowed)\nGreatest relative difference: inf at index (1, 1, 25, 7) (up to 0.001 allowed)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 속도 비교"
      ],
      "metadata": {
        "id": "b55M-IbjZ7at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
        "        x_vals=[128 * i for i in range(2, 16)],  # different possible values for `x_name`\n",
        "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
        "        line_vals=[\n",
        "            'triton',\n",
        "            'torch-native',\n",
        "        ],  # possible values for `line_arg``\n",
        "        line_names=[\n",
        "            \"Triton\",\n",
        "            \"Torch (native)\",\n",
        "        ],  # label name for the lines\n",
        "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
        "        ylabel=\"GB/s\",  # label name for the y-axis\n",
        "        plot_name=\"RoPE-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
        "    ))\n",
        "def benchmark(M, N, provider):\n",
        "    x = torch.randn(N, M, 64, 1, device='cuda', dtype=torch.float32)\n",
        "    x = x.transpose(0, 1).contiguous()\n",
        "    rotary_pos_emb = RotaryPositionEmbedding(1, 0.5)\n",
        "    emb = rotary_pos_emb(4096)\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    if provider == 'torch-native':\n",
        "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: apply_rotary_pos_emb(x, emb, fused = False), quantiles=quantiles)\n",
        "    if provider == 'triton':\n",
        "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: apply_rotary_pos_emb(x, emb, fused = True), quantiles=quantiles)\n",
        "    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
        "\n",
        "\n",
        "benchmark.run(show_plots=True, print_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "c_2jNAcKxmOa",
        "outputId": "1a7b8f58-f522-44e7-e0c7-1096ba58f23e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoPE-performance:\n",
            "         N     Triton  Torch (native)\n",
            "0    256.0  20.787783      201.993981\n",
            "1    384.0  26.144517      201.128691\n",
            "2    512.0  25.043844      200.033580\n",
            "3    640.0  26.086072      201.590592\n",
            "4    768.0  25.880261      201.109794\n",
            "5    896.0  25.550808      200.303785\n",
            "6   1024.0  25.440984      200.307018\n",
            "7   1152.0  25.232118      199.838726\n",
            "8   1280.0  25.289317      200.324003\n",
            "9   1408.0  23.017107      198.999748\n",
            "10  1536.0  25.429889      199.273285\n",
            "11  1664.0  25.179261      199.109545\n",
            "12  1792.0  25.245914      199.354122\n",
            "13  1920.0  25.444337      199.818675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCykdZozMFX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
